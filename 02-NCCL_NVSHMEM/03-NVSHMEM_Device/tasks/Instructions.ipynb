{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hands-On 3 (optional): Device-initiated Communication with NVSHMEM\n",
        "\n",
        "## Task: Using the NVSHMEM device API\n",
        "\n",
        "### Description\n",
        "\n",
        "In our last hands-on, we use the NVSHMEM device API instead of MPI to\n",
        "implement a multi-GPU Jacobi solver. The starting point of this task is\n",
        "the MPI variant of the Jacobi solver. You need to work on `TODOs` in\n",
        "`jacobi.cu`:\n",
        "\n",
        "-   Initialize NVSHMEM (this is same as in previous hands-on 2):\n",
        "    -   Include NVSHMEM headers.\n",
        "    -   Initialize and shutdown NVSHMEM using `MPI_COMM_WORLD`.\n",
        "    -   Allocate work arrays `a` and `a_new` from the NVSHMEM symmetric\n",
        "        heap. Again, ensure the size you pass in is the same value on\n",
        "        all participating ranks!\n",
        "    -   Calculate halo/boundary row index of top and bottom neighbors.\n",
        "    -   Add necessary inter PE synchronization.\n",
        "-   Modify `jacobi_kernel` (this is the device-initiated part):\n",
        "    -   Pass in halo/boundary row index of top and bottom neighbors.\n",
        "    -   Use `nvshmem_float_p` to directly push values needed by top and\n",
        "        bottom neighbors from the kernel.\n",
        "    -   Remove no longer needed MPI communication.\n",
        "\n",
        "Compile with\n",
        "\n",
        "``` bash\n",
        "make\n",
        "```\n",
        "\n",
        "Submit your compiled application to the batch system with\n",
        "\n",
        "``` bash\n",
        "make run\n",
        "```\n",
        "\n",
        "Study the performance by inspecting the profile generated with\n",
        "`make profile`. For `make run` and `make profile`, the environment\n",
        "variable `NP` can be set to change the number of processes.\n",
        "\n",
        "#### Note\n",
        "\n",
        "The Slurm installation on the JSC systems sets `CUDA_VISIBLE_DEVICES`\n",
        "automatically so that each spawned process only sees the GPU it should\n",
        "use (see e.g.Â [GPU\n",
        "Devices](https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html#gpu-devices)\n",
        "in the JUWELS Booster Overview documentation).\n",
        "\n",
        "This is not supported for NVSHMEM. The automatic setting of\n",
        "`CUDA_VISIBLE_DEVICES` can be disabled by setting\n",
        "`CUDA_VISIBLE_DEVICES=0,1,2,3` in the shell that executes `srun`.\n",
        "\n",
        "With `CUDA_VISIBLE_DEVICES` set explicitly, all spawned processes can\n",
        "see all GPUs listed. This is automatically done for the `sanitize`,\n",
        "`run` and `profile` make targets.\n",
        "\n",
        "## Advanced Task: Use `nvshmemx_float_put_nbi_block`\n",
        "\n",
        "### Description\n",
        "\n",
        "This is an optional task to use `nvshmemx_float_put_nbi_block` instead\n",
        "of `nvshmem_float_p` for more efficient multi node execution. There are\n",
        "no `TODO`s prepared. Use the solution of the previous task as a starting\n",
        "point. Some tips:\n",
        "\n",
        "-   You only need to change `jacobi_kernel`.\n",
        "-   Switching to a 1-dimensional CUDA block can simplify the task.\n",
        "-   The difficult part is calculating the right offsets and size for\n",
        "    calling into `nvshmemx_float_put_nbi_block`.\n",
        "-   If a CUDA blocks needs to communicate data with\n",
        "    `nvshmemx_float_put_nbi_block`, all threads in that block need to\n",
        "    call into `nvshmemx_float_put_nbi_block`.\n",
        "-   The\n",
        "    [`nvshmem_opt`](https://github.com/NVIDIA/multi-gpu-programming-models/blob/master/nvshmem_opt/jacobi.cu#L154)\n",
        "    variant in the [Multi GPU Programming Models Github\n",
        "    repository](https://github.com/NVIDIA/multi-gpu-programming-models)\n",
        "    implements the same strategy."
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  }
}
